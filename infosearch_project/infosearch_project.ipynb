{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения текста вопросов.\n",
    "def getting_texts():\n",
    "    # Для ускоренной настройки модели считывается только 1000 первых строчек, но можно указать 'None' для чтения всего файла.\n",
    "    nRowsRead = 10001\n",
    "    data = pd.read_csv(\"quora_question_pairs_rus.csv\", delimiter=',', header = 0, nrows = nRowsRead).dropna()\n",
    "    all_questions_dataset = pd.concat([data['question1'], data['question2']], ignore_index=True)\n",
    "    all_questions_list = all_questions_dataset.tolist()\n",
    "    \n",
    "    return all_questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем лемматизатор, устанавливаем стоп-слова и немного расширяем пунктуацию.\n",
    "morph_analyze = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "punctuation += '«»…—'\n",
    "\n",
    "# Функция для препроцессинга текста\n",
    "def preprocessing_text(text):\n",
    "    lemmas = []\n",
    "    # Переводим всё в нижний регистр шрифта и делим по пробелам\n",
    "    pre_tokens = word_tokenize(text.lower())\n",
    "    # Избавляемся от стоп-слов и пунктуации, лемматизируем\n",
    "    for one_pre_token in pre_tokens:\n",
    "        if one_pre_token not in punctuation and one_pre_token not in russian_stopwords:\n",
    "            one_lemma = morph_analyze.normal_forms(one_pre_token)[0]\n",
    "            lemmas.append(one_lemma)\n",
    "    # Превращаем в строки с леммами, разделёнными пробелами\n",
    "    processed_text = ' '.join(lemmas)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая создаёт последовательности: исходные вопросы из датасета, они же после препроцессинга, их id.\n",
    "def sequencing_texts(texts_list):\n",
    "    # Вспомогательная переменная, здесь будут храниться все тексты, прошедшие предпроцессинг.\n",
    "    all_questions_processed = []\n",
    "    # А здесь будут храниться все тексты, которые не прошли предпроцессинг.\n",
    "    all_questions_raw = texts_list\n",
    "    for one_question in all_questions_raw:\n",
    "        # Лемматизируем текст вопроса, избавляемся от стоп-слов и ненужных символов.\n",
    "        processed_question = preprocessing_text(one_question)\n",
    "        # Записываем прошедший предобработку вопрос в переменную.\n",
    "        all_questions_processed.append(processed_question)\n",
    "    # Делаем массив с id вопросов.\n",
    "    all_question_ids = [i for i in range(1, len(all_questions_processed) + 1)]\n",
    "    return all_questions_raw, all_questions_processed, all_question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN. Делаем первичную обработку данных, представляем их в удобном виде.\n",
    "\n",
    "all_questions_raw, all_questions_processed, all_question_ids = sequencing_texts(getting_texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Создаём матрицу TF-IDF.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_questions_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing():\n",
    "    query = input('Введите строку запроса: ')\n",
    "    processed_query = preprocessing_text(query)\n",
    "    query_list = processed_query.split()\n",
    "    query_matrix = tfidf_vectorizer.transform(query_list)\n",
    "    return query_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching_cosine_similarity(query_matrix):\n",
    "    cosine_similarities = linear_kernel(query_matrix[0:1], tfidf).flatten()\n",
    "    related_questions_ids = cosine_similarities.argsort()[:-6:-1]\n",
    "    related_questions_metrics = cosine_similarities[related_questions_ids]\n",
    "    res_number = 1\n",
    "    for i in related_questions_ids:\n",
    "        metr = related_questions_metrics[res_number-1]\n",
    "        print('№ ' + str(res_number) + ', метрика = ' + str(metr) + ', вопрос: ' + all_questions_raw[i])\n",
    "        res_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите строку запроса: в чем смысл жизни\n",
      "№ 1, метрика = 0.8080761171645133, вопрос: в чем смысл жизни\n",
      "№ 2, метрика = 0.8080761171645133, вопрос: какой смысл этой жизни\n",
      "№ 3, метрика = 0.8080761171645133, вопрос: в чем смысл жизни\n",
      "№ 4, метрика = 0.8080761171645133, вопрос: какой смысл этой жизни\n",
      "№ 5, метрика = 0.8080761171645133, вопрос: в чем смысл жизни\n"
     ]
    }
   ],
   "source": [
    "searching_cosine_similarity(query_preprocessing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
