{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "# from collections import Counter, defaultdict\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения списка адресов нужных файлов\n",
    "def getting_documents():\n",
    "    subtitles_directory = os.getcwd() + \"\\\\friends\"\n",
    "    seasons_list = []\n",
    "    documents_list = []\n",
    "    # получение списка папок по сезонам\n",
    "    for folder in os.listdir(subtitles_directory):\n",
    "        seasons_list.append(subtitles_directory + '\\\\' + folder)\n",
    "    # обход каждой папки с сезонами\n",
    "    for one_folder in seasons_list:\n",
    "        for text_file in os.listdir(one_folder):\n",
    "            documents_list.append(one_folder + '\\\\' + text_file)\n",
    "    # на выходе получается общий список\n",
    "    return documents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настраиваем лемматизатор, устанавливаем стоп-слова и немного расширяем пунктуацию.\n",
    "morph_analyze = pymorphy2.MorphAnalyzer()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "punctuation += '«»…—'\n",
    "\n",
    "# Функция для препроцессинга текста\n",
    "def preprocessing_text(text):\n",
    "    lemmas = []\n",
    "    # Переводим всё в нижний регистр шрифта и делим по пробелам\n",
    "    pre_tokens = word_tokenize(text.lower())\n",
    "    # Избавляемся от стоп-слов и пунктуации, лемматизируем\n",
    "    for one_pre_token in pre_tokens:\n",
    "        if one_pre_token not in punctuation and one_pre_token not in russian_stopwords:\n",
    "            one_lemma = morph_analyze.normal_forms(one_pre_token)[0]\n",
    "            lemmas.append(one_lemma)\n",
    "    # Превращаем в строки с леммами, разделёнными пробелами\n",
    "    processed_text = ' '.join(lemmas)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные списки для дальнейшего создания матриц\n",
    "all_texts = []\n",
    "all_episodes = []\n",
    "\n",
    "for one_document in getting_documents():\n",
    "    # Открываем по очереди все текстовые файлы и считываем текст из них\n",
    "    with open(one_document, \"r\", encoding=\"utf-8\") as f:\n",
    "        original_text = f.read()\n",
    "    # Проводим предпроцессинг текста\n",
    "    processed_text = preprocessing_text(original_text)\n",
    "    all_texts.append(processed_text)\n",
    "    # Вытаскиваем из названия файла номер эпизода с помощью регулярного выражения\n",
    "    episode_number = re.search(r'\\dx\\d+(-\\d\\d)*', one_document).group(0)\n",
    "    all_episodes.append(episode_number)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
